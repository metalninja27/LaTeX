\documentclass[11pt]{article}

% packages
\usepackage{physics}
% margin spacing
\usepackage[top=1in, bottom=1in, left=0.5in, right=0.5in]{geometry}
\usepackage{hanging}
\usepackage{amsfonts, amsmath, amssymb, amsthm}
\usepackage{systeme}
\usepackage[none]{hyphenat}
\usepackage{fancyhdr}
\usepackage[nottoc, notlot, notlof]{tocbibind}
\usepackage{graphicx}
\graphicspath{{./images/}}
\usepackage{float}
\usepackage{siunitx}
\usepackage{esint}
\usepackage{cancel}

% colors
\usepackage{xcolor}
\definecolor{p}{HTML}{FFDDDD}
\definecolor{g}{HTML}{D9FFDF}
\definecolor{y}{HTML}{FFFFCF}
\definecolor{b}{HTML}{D9FFFF}
\definecolor{o}{HTML}{FADECB}
%\definecolor{}{HTML}{}

% \highlight[<color>]{<stuff>}
\newcommand{\highlight}[2][p]{\mathchoice%
  {\colorbox{#1}{$\displaystyle#2$}}%
  {\colorbox{#1}{$\textstyle#2$}}%
  {\colorbox{#1}{$\scriptstyle#2$}}%
  {\colorbox{#1}{$\scriptscriptstyle#2$}}}%

% header/footer formatting
\pagestyle{fancy}
\fancyhead{}
\fancyfoot{}
\fancyhead[L]{MAS4105 Dr. Zhang}
\fancyhead[C]{HW14}
\fancyhead[R]{Sai Sivakumar}
\fancyfoot[R]{\thepage}
% remove underlined header
%\renewcommand{\headrulewidth}{0pt}

% paragraph indentation/spacing
\setlength{\parindent}{0cm}
\setlength{\parskip}{5pt}
\renewcommand{\baselinestretch}{1.25}

% extra commands defined here
\newcommand{\ihat}{\boldsymbol{\hat{\textbf{\i}}}}
\newcommand{\jhat}{\boldsymbol{\hat{\textbf{\j}}}}
\newcommand{\dr}{\vec{r}~^{\prime}(t)}
\newcommand{\dx}{x^{\prime}(t)}
\newcommand{\dy}{y^{\prime}(t)}

\newcommand{\br}[1]{\left(#1\right)}
\newcommand{\sbr}[1]{\left[#1\right]}
\newcommand{\cbr}[1]{\left\{#1\right\}}
\newcommand{\abr}[1]{\left\langle#1\right\rangle}

\newcommand{\dprime}{\prime\prime}
\newcommand{\lap}[2]{\mathcal{L}[#1](#2)}

% mathtools
\usepackage{mathtools}

%\DeclarePairedDelimiterX{\abr}[1]{\langle}{\rangle}{#1}

\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}

\DeclareMathOperator{\Span}{span}
\DeclareMathOperator{\nullity}{nullity}

% set page count index to begin from 1
\setcounter{page}{1}

% theorem/corollary/lemma
\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{lemma}[theorem]{Lemma}

\newcommand{\vardbtilde}[1]{\tilde{\raisebox{0pt}[0.85\height]{$\tilde{#1}$}}}

\newtheorem*{theorem*}{Theorem}

\begin{document}
5.2: 8

5.4: 6 (a,b), 18

6.1: 9, 10, 11, 12, 13, 15, 17 \\

5.2:

8. Suppose that $A\in\mathsf{M}_{n\times n}(\mathbb{F})$ has two distinct eigenvalues, $\lambda_1$ and $\lambda_2$, and that $\dim(\mathsf{E}_{\lambda_1}) = n-1$. Prove that $A$ is diagonalizible.

\begin{proof}
  Suppose that $A$ has two distinct eigenvalues, $\lambda_1$ and $\lambda_2$, and that $\dim(\mathsf{E}_{\lambda_1}) = n-1$. Then we also know that $\dim(\mathsf{E}_{\lambda_2}) \geq 1$, because $\mathsf{E}_{\lambda_2}$ is a nontrivial subspace of $\mathbb{F}^n$ (so it contains at least one nonzero vector). 
  
  Since the dimension of the eigenspace corresponding to $\lambda_1$ is $n-1$, there are $n-1$ linearly independent eigenvectors corresponding to $\lambda_1$. Similarly, there is at least one linearly independent (nonzero) eigenvector in the eigenspace corresponding to $\lambda_2$, but because these eigenvectors are also linearly independent to those eigenvectors which form the basis $\beta$ for $\mathsf{E}_{\lambda_1}$, then we can extend $\beta$ by exactly one vector found in $\mathsf{E}_{\lambda_2}$ to form a complete basis for $\mathbb{F}^n$. This means that there cannot be any more eigenvectors for $A$, and so in actuality, $\dim(\mathsf{E}_{\lambda_2}) = 1$.

  Also remember that the algebraic multiplicity for eigenvalues is greater than or equal to the geometric multiplicity for eigenvalues (dimension of eigenspace corresponding to an eigenvalue). Then also remember that the sum of the algebraic multiplicities is equal to the degree of the characteristic polynomial. Recall that the degree of the characteristic polynomial for $A$ is $n$ since $A$ is an $n\times n$ matrix. 

  So let $a$ and $b$ be the algebraic multiplicities for eigenvalues $\lambda_1$ and $\lambda_2$, respectively. Then $a\geq n-1$ and $b\geq 1$, but $a+b = n$, so that $n-1 + 1 \leq a+b = n \iff n \leq n \iff a=n-1, b=1$. Hence the algebraic multiplicities are equal to the geometric multiplicities for each eigenvalue, so by Theorem 5.9 we have that $A$ is diagonalizible.
\end{proof}

5.4:

6. For each linear operator $\mathsf{T}$ on the vector space $\mathsf{V}$, find an ordered basis for the $\mathsf{T}$-cyclic subspace generated by the vector $z$.

(a) $\mathsf{V} = \mathbb{R}^4$, $\mathsf{T}(a,b,c,d) = (a+b,b-c,a+c,a+d)$, and $z=e_1$.

To find a basis we can start with $z$ and repeatedly apply $\mathsf{T}$ on it to generate vectors until we find one that is a linear combination of the previous vectors. All of these vectors (except for the last linearly dependent one) then form a basis for the $\mathsf{T}$-cyclic subspace generated by the vector $z$. \begin{align*}
  z &= (1,0,0,0) \\
  \mathsf{T}(z) &= (1,0,1,1) \\
  \mathsf{T}(1,0,1,1) &= (1,-1,2,2) \\
  \mathsf{T}(1,-1,2,2) &= (0,-3, 3, 3)
\end{align*}

Notice that the last vector, $(0,-3,3,3)$ can be written as $3\sbr{(1,-1,2,2) - (1,0,1,1)}$. So the first three vectors form the basis $\cbr{z,\mathsf{T}(z), \mathsf{T}^2(z)} = \cbr{(1,0,0,0), (1,0,1,1), (1,-1,2,2)}$.

(b) $\mathsf{V} = \mathsf{P}_3(\mathbb{R})$, $\mathsf{T}(f(x)) = f^{\dprime}(x)$, and $z=x^3$.

Same as before. \begin{align*}
  z &= x^3 \\
  \mathsf{T}(x^3) &= 6x \\
  \mathsf{T}(6x) &= 0 \\
\end{align*}

We can halt there and just use the vectors seen above to give a basis as $\cbr{6x,x^3}$ (we can reduce $6x$ to $x$ and still have a basis).

18. Let $A$ be an $n\times n$ matrix with characteristic polynomial $$f(t) = (-1)^n t^n+a_{n-1}t^{n-1}+ \cdots + a_1t+a_0.$$

(a) Prove that $A$ is invertible if and only if $a_0\neq 0$.

\begin{proof}
  Suppose $a_0\neq 0$. Then it is clear that $(t-0)$ is not a factor of the polynomial, which is equivalent to saying that $t=0$ is not a root to the polynomial, meaning that $\lambda=0$ is not an eigenvalue. Thus there are no nonzero vectors $v$ which satisfy $Av = 0v = \vec{0}$, which means that the kernel of $A$ contains only the zero vector. By the rank nullity theorem we must have that the linear operator encoded by $A$ (so $\mathsf{L}_A$) is injective and onto, and hence invertible.

  Conversely, suppose that $A$ is invertible so that there are no nonzero vectors $v$ which satisfy $Av = \vec{0} = 0v$, which means that $0$ is not an eigenvalue and will not be a root for the characteristic polynomial. By the fundamental theorem of algebra, we have that $(t-0)$ will then not be a factor for the characteristic polynomial. This is equivalent to saying that the characteristic polynomial contains a nonzero constant term (so that $(t-0)$ does not divide this polynomial).

  Hence $A$ is invertible if and only if $a_0\neq 0$.
\end{proof}

(b) Prove that if $A$ is invertible, then $$A^{-1} = \frac{-1}{a_0}\sbr{(-1)^nA^{n-1}+a_{n-1}A^{n-2}+\cdots +a_1I_n}.$$

\begin{proof}
  Suppose that $A$ is invertible. Then we use the matrix given and show that $AA^{-1} = I_n$ (since inverses are unique). 
  \begin{align*}
    AA^{-1} &= A\br{\frac{-1}{a_0}\sbr{(-1)^nA^{n-1}+a_{n-1}A^{n-2}+\cdots +a_1I_n}} \\
    &= \frac{-1}{a_0}\sbr{(-1)^nAA^{n-1}+a_{n-1}AA^{n-2}+\cdots +a_1AI_n} \\
    &= \frac{-1}{a_0}\sbr{(-1)^nA^{n}+a_{n-1}A^{n-1}+\cdots +a_1A + a_0I_n - a_0I_n} \\
    &= \frac{-1}{a_0}\sbr{\br{(-1)^nA^{n}+a_{n-1}A^{n-1}+\cdots +a_1A^1 + a_0A^0} - a_0I_n} \\
    \intertext{By the Cayley-Hamilton Theorem, $\br{(-1)^nA^{n}+a_{n-1}A^{n-1}+\cdots +a_1A + a_0} = f(A) = \mathsf{T}_0$, where $\mathsf{T}_0$ is the zero operator.}
    &= \frac{-1}{a_0}\br{\mathsf{T}_0 - a_0I_n} \\
    &= \frac{-1}{a_0}\mathsf{T}_0 - \frac{-1}{a_0}a_0I_n \\
    &= \mathsf{T}_0 + I_n = \boxed{I_n}
  \end{align*}

  Hence $A^{-1} = \frac{-1}{a_0}\sbr{(-1)^nA^{n-1}+a_{n-1}A^{n-2}+\cdots +a_1I_n}$.
\end{proof}

(c) Use (b) to compute $A^{-1}$ for $$A = \begin{pmatrix}
  1 & 2 & 1 \\
  0 & 2 & 3 \\
  0 & 0 & -1
\end{pmatrix}.$$

First we find the characteristic polynomial $f(t) = \det(A-tI_3)$ for $A$. \begin{align*}
  f(t) = \det(A-tI_3) &= \det\begin{pmatrix}
  1-t & 2 & 1 \\
  0 & 2-t & 3 \\
  0 & 0 & -1-t
\end{pmatrix}\\
&= (-1)^3(t-1)(t-2)(t+1) \\
&= (-1)t^3+2t^2+t-2
\end{align*}

So in the formula given in (b), let $n=3$, $a_0=-2$, $a_1 = 1$, $a_2 = 2$. Then \begin{align*}
  A^{-1} &= \frac{-1}{-2}\sbr{(-1)A^{2}+2A +I_n} \\
  &= \frac{1}{2}\sbr{(-1)\begin{pmatrix}
    1 & 2 & 1 \\
    0 & 2 & 3 \\
    0 & 0 & -1
  \end{pmatrix}\begin{pmatrix}
    1 & 2 & 1 \\
    0 & 2 & 3 \\
    0 & 0 & -1
  \end{pmatrix} + 2\begin{pmatrix}
    1 & 2 & 1 \\
    0 & 2 & 3 \\
    0 & 0 & -1
  \end{pmatrix} + \begin{pmatrix}
    1 & 0 & 0 \\
    0 & 1 & 0 \\
    0 & 0 & 1
  \end{pmatrix}} \\
  &= \frac{1}{2}\sbr{\begin{pmatrix}
    -1 & -6 & -6 \\
    0 & -4 & -3 \\
    0 & 0 & -1
  \end{pmatrix} + \begin{pmatrix}
    3 & 4 & 2 \\
    0 & 5 & 6 \\
    0 & 0 & -1
  \end{pmatrix}} \\
  &= \boxed{\frac{1}{2}\begin{pmatrix}
    2 & -2 & -4 \\
    0 & 1 & 3 \\
    0 & 0 & -2
  \end{pmatrix}}
\end{align*}

6.1:

9. Let $\beta$ be a basis for a finite dimensional inner product space.

(a) Prove that if $\abr{x,z} = 0$ for all $z\in\beta$, then $x=\vec{0}$.

\begin{proof}
  Suppose by way of contradiction that $x$ is a nonzero vector that satisfies $\abr{x,z} = 0$ for all $z\in\beta$, and suppose $\abs{\beta} = n$. 
  
  Then observe that we may write $x$ as a linear combination of the vectors in the basis; that is, since $x\in\Span(\beta)$, we have $x = c_1z_1 + c_2z_2 + \cdots + c_nz_n$. Then since the inner product is linear in the first component, we can write $$\abr{x,z} = \sum_{k=1}^nc_k\abr{z_k,z} = 0.$$ Then pull out one item of the form $c_k\abr{z_k,z}$ from the sum with $c_k\neq 0$ (since $x$ is a nonzero vector). So without loss of generality, for some $j\in N$, where $N = \cbr{1,2,\dots,n}$, we have $$\abr{x,z} = \sum_{k=1}^nc_k\abr{z_k,z} = c_j\abr{z_j,z} + \sum\limits_{\substack{k\in N \\ k\neq j}}c_k\abr{z_k,z} = \abr{c_jz_j,z} + \abr{\sum\limits_{\substack{k\in N \\ k\neq j}}c_kz_k,z} = 0,$$ where $c_j\neq 0$.

  %Since this equation is true for all $z\in\beta$, we can consider a system of equations found by choosing all of the different values that $z$ can take on in $\beta$. Let $N = \cbr{1,2,\dots,n}$.
  %\begin{align*}
  %  \sum_{k=1}^nc_k\abr{z_k,z_1} = c_1\abr{z_1,z_1} + \sum\limits_{\substack{k\in N \\ k\neq 1}}c_k\abr{z_k,z_1} &= 0 \\
  %  \sum_{k=1}^nc_k\abr{z_k,z_2} = c_2\abr{z_2,z_2} + \sum\limits_{\substack{k\in N \\ k\neq 2}}c_k\abr{z_k,z_2} &= 0 \\
  %  \vdots\hspace{3cm} \\
  %  \sum_{k=1}^nc_k\abr{z_k,z_n} = c_n\abr{z_n,z_n} + \sum\limits_{\substack{k\in N \\ k\neq n}}c_k\abr{z_k,z_n} &= 0 \\
  %\end{align*}
  
  Conjugate all items in the equality: $$\overline{\abr{c_jz_j,z} + \abr{\sum\limits_{\substack{k\in N \\ k\neq j}}c_kz_k,z}} = \overline{\abr{c_jz_j,z}} + \overline{\abr{\sum\limits_{\substack{k\in N \\ k\neq j}}c_kz_k,z}} = \abr{z,c_jz_j} + \abr{z,\sum\limits_{\substack{k\in N \\ k\neq j}}c_kz_k} = \overline{0} = 0.$$

  Then $$\abr{z,\sum\limits_{\substack{k\in N \\ k\neq j}}c_kz_k} = (-1)\abr{z,c_jz_j} = \overline{(-1)}\abr{z,c_jz_j} = \abr{z,-c_jz_j},$$ which implies (by another property of the inner product) that $$-c_jz_j = \sum\limits_{\substack{k\in N \\ k\neq j}}c_kz_k \iff z_j = \frac{c_1}{c_j}z_1 + \frac{c_2}{c_j}z_2 + \cdots + \frac{c_n}{c_j}z_n,$$ where $z_j$ does not appear on the right hand side. So we have expressed $z_j$ as a linear combination of the other vectors in $\beta$, which is in contradiction with $\beta$ being linearly independent ($\beta$ is a basis). 
  
  Hence we cannot have that $x$ is a nonzero vector, so $x = \vec{0}$.
\end{proof}

(b) Prove that if $\abr{x,z} = \abr{y,z}$ for all $z\in \beta$, then $x=y$.

\begin{proof}
  Suppose $\abr{x,z} = \abr{y,z}$ for all $z\in \beta$. Then subtract from both sides $\abr{y,z}$. Then use the linearity of the inner product in the first component and part (a) to find that \\ $\abr{x,z} - \abr{y,z} = \abr{x-y,z} = 0 \iff x-y = \vec{0} \iff x=y$. 
\end{proof}

10.$^{\dagger}$ Let $\mathsf{V}$ be an inner product space, and suppose that $x$ and $y$ are orthogonal vectors in $\mathsf{V}$. Prove that $\norm{x+y}^2 = \norm{x}^2 + \norm{y}^2$. Deduce the Pythagorean theorem in $\mathbb{R}^2$.

\begin{proof}
  Suppose that $x$ and $y$ are orthogonal vectors in $\mathsf{V}$. Then by definition, $\abr{x,y} = 0$ and also $\norm{x+y} = \sqrt{\abr{x+y,x+y}}$.

  Then from the linearity of the inner product and the previous statements, 
  \begin{align*}
    \norm{x+y}^2 &= \abr{x+y,x+y} \\
    &= \abr{x,x+y} + \abr{y,x+y} \\
    &= \abr{x,x} + \abr{x,y} + \abr{y,x} + \abr{y,y} \hspace{1cm}(\text{since }\overline{1} = 1) \\
    &= \abr{x,x} + 0 + 0 + \abr{y,y} \\
    &= \norm{x}^2 + \norm{y}^2.
  \end{align*}

  Then let $\mathsf{V}=\mathbb{R}^2$. So for any \textit{orthogonal} vectors $a,b\in\mathbb{R}^2$, write $a = a_1e_1 + a_2e_2$ and $b = b_1e_1 + b_2e_2$. We know that since $\abr{e_1,e_2} = e_1\cdot e_2 = 0$, we have that $e_1,e_2$ are orthogonal, and we also have that $\abr{a,b} = a\cdot b = (a_1e_1 + a_2e_2)\cdot (b_1e_1 + b_2e_2) = a_1b_1(e_1\cdot e_1) + a_1b_2(e_1\cdot e_2) + a_2b_1(e_2\cdot e_1) + a_2b_2(e_2\cdot e_2) = a_1b_1 + a_2b_2 = 0$.
  
  Then \begin{align*}
    \norm{a+b}^2 = \abr{a+b,a+b} &= (a+b)\cdot (a+b) \\
    &= (a_1e_1 + a_2e_2 + b_1e_1 + b_2e_2) \cdot (a_1e_1 + a_2e_2 + b_1e_1 + b_2e_2) \\
    &= \br{(a_1+b_1)e_1 + (a_2+b_2)e_2}\cdot \br{(a_1+b_1)e_1 + (a_2+b_2)e_2} \\
    &= 
    (a_1+b_1)e_1 \cdot (a_1+b_1)e_1 + (a_2+b_2)e_2 \cdot (a_1+b_1)e_1 \\ &\hspace{1cm}+ (a_1+b_1)e_1 \cdot (a_2+b_2)e_2 + (a_2+b_2)e_2 \cdot (a_2+b_2)e_2 \\
    &=(a_1+b_1)^2(e_1\cdot e_1) + 2\br{(a_1+b_1)(a_2+b_2)}(e_1\cdot e_2) + (a_2+b_2)^2(e_2\cdot e_2) \\
    &= 1(a_1+b_1)^2 + 0 + 1(a_2+b_2)^2 \\
    &= a_1^2+a_2^2 + b_1^2+b_2^2 +2(a_1b_1 + a_2b_2) \\
    &= (a\cdot a) + (b\cdot b) + 0 \\
    &= \norm{a}^2+\norm{b}^2
  \end{align*}

  This concludes the proof.
\end{proof}

11. Prove the \textit{parallelogram law} on an inner product space $\mathsf{V}$; that is, show that $$\norm{x+y}^2 + \norm{x-y}^2 = 2\norm{x}^2 + 2\norm{y}^2 \hspace{1cm}\text{for all }x,y\in\mathsf{V}.$$ What does this equation state about parallelograms in $\mathbb{R}^2$?

\begin{proof}
  Let $x,y\in\mathsf{V}$. Then \begin{align*}
    \norm{x+y}^2 + \norm{x-y}^2 &= \abr{x+y,x+y} + \abr{x-y,x-y} \\
    &= \abr{x,x} + \abr{x,y} + \abr{y,x} + \abr{y,y} + \abr{x,x} + \abr{x,-y} + \abr{-y,x} + \abr{-y,-y}\\
    &= \highlight[p]{2\abr{x,x}} + \abr{x,y} + \abr{y,x} + \highlight[g]{2\abr{y,y}} + \abr{x,-y} + \abr{-y,x} \hspace{1cm}(\text{since } \abr{-y,-y} = (-1)^2\abr{y,y})\\
    &= \highlight[p]{2\norm{x}^2} + \highlight[g]{2\norm{y}^2}+ \abr{x,y} + \abr{y,x} + (-1)\abr{x,y} + (-1)\abr{y,x} \\
    &= 2\norm{x}^2 + 2\norm{y}^2
  \end{align*}

  In $\mathbb{R}^2$, this states that summing the squared lengths of the diagonals of parallograms is equivalent to squaring the side lengths of the parallelogram and doubling the result. So you can draw the addition of vectors (via tip-to-tail) in $\mathbb{R}^2$ as forming a parallelogram and so the lengths of the parallelogram as described above satisfy the equality in the law.
\end{proof} \newpage

12.$^{\dagger}$ Let $\cbr{v_1,v_2,\dots,v_k}$ be an orthogonal set in $\mathsf{V}$, and let $a_1,a_2,\dots,a_k$ be scalars. Prove that $$\norm{\sum_{i=1}^k a_iv_i}^2 = \sum_{i=1}^k \abs{a_i}^2\norm{v_i}^2.$$

\begin{proof}
  Let $\cbr{v_1,v_2,\dots,v_k}$ be an orthogonal set in $\mathsf{V}$, and let $a_1,a_2,\dots,a_k$ be scalars. Then \begin{align*}
    \norm{\sum_{i=1}^k a_iv_i}^2 &= \abr{\sum_{i=1}^k a_iv_i, \sum_{i=1}^k a_iv_i}\\
    &= \sum_{r=0}^k\br{\sum_{s=0}^k\br{\abr{a_sv_s, a_rv_r}}} \\
    \intertext{All items of the form $\abr{a_iv_i,a_jv_j}$ where $i\neq j$ vanish due to $\cbr{v_1,v_2,\dots,v_k}$ being an orthogonal set in $\mathsf{V}$.} &= \sum_{i=1}^k \br{\abr{a_iv_i,a_iv_i}} \\
    &= \sum_{i=1}^k\br{a_i\overline{a_i}\abr{v_i,v_i}} \\
    &= \sum_{i=1}^k \abs{a_i}^2\norm{v_i}^2
  \end{align*}

  Hence $$\norm{\sum_{i=1}^k a_iv_i}^2 = \sum_{i=1}^k \abs{a_i}^2\norm{v_i}^2$$ holds when $\cbr{v_1,v_2,\dots,v_k}$ is an orthogonal set in $\mathsf{V}$.
\end{proof}

13. Suppose that $\abr{\cdot,\cdot}_1$ and $\abr{\cdot,\cdot}_2$ are two inner products on a vector space $\mathsf{V}$. Prove that $\abr{\cdot,\cdot} = \abr{\cdot,\cdot}_1 + \abr{\cdot,\cdot}_2$ is another inner product on $\mathsf{V}$.

\begin{proof}
  Suppose $\abr{\cdot,\cdot}_1$ and $\abr{\cdot,\cdot}_2$ are two inner products on $\mathsf{V}$. Then define $\abr{\cdot,\cdot}$ by $\abr{\cdot,\cdot} = \abr{\cdot,\cdot}_1 + \abr{\cdot,\cdot}_2$.

  %\abr{,} = \abr{,}_1 + \abr{,}_2

  To show that this is another inner product on $\mathsf{V}$, we have to verify four conditions. Let $x,y,z\in\mathsf{V}$ and $c\in\mathbb{F}$.

  \hspace{2cm}(a)$\abr{x+z,y} = \abr{x,y} + \abr{z,y}$. \hspace{2cm}(b) $\abr{cx,y} = c\abr{x,y}$.

  \hspace{2cm}(c) $\overline{\abr{x,y}} = \abr{y,x}$. \hspace{3.9cm}(d) $\abr{x,x}> 0$ if $x\neq 0$.

  (a) $\abr{x+z,y} = \abr{x+z,y}_1 + \abr{x+z,y}_2 = \abr{x,y}_1 + \abr{z,y}_1 + \abr{x,y}_2 + \abr{z,y}_2 = \br{\abr{x,y}_1 + \abr{x,y}_2} + \br{\abr{z,y}_1 + \abr{z,y}_2} = \abr{x,y} + \abr{z,y}$.

  (b) $\abr{cx,y} = \abr{cx,y}_1 + \abr{cx,y}_2 = c\abr{x,y}_1 + c\abr{x,y}_2 = 2\br{\abr{x,y}_1 + \abr{x,y}_2} = c\abr{x,y}$.

  (c) $\overline{\abr{x,y}} = \overline{\abr{x,y}_1+\abr{x,y}_2} = \overline{\abr{x,y}_1} + \overline{\abr{x,y}_2} = \abr{y,x}_1 + \abr{y,x}_2 = \abr{y,x}$.

  (d) $\abr{x,x} = \abr{x,x}_1 + \abr{x,x}_2 > 0$ since $\abr{x,x}_1>0$ and $\abr{x,x}_2>0$.

  Hence $\abr{\cdot,\cdot} = \abr{\cdot,\cdot}_1 + \abr{\cdot,\cdot}_2$ is another inner product on $\mathsf{V}$.
\end{proof}

15. 

(a) Prove that if $\mathsf{V}$ is an inner product space, then $\abs{\abr{x,y}} = \norm{x}\cdot \norm{y}$ if and only if one of the vectors $x$ or $y$ is a multiple of the other. 

\begin{proof}
  The theorem holds when either vector is zero, in both directions. For the rest of the proof, suppose that $y\neq \vec{0}$.

  Without loss of generality, suppose that $x$ is a multiple of $y$, so that for some scalar $c$ (not complex), $x=cy$. Then \begin{multline*}\abs{\abr{x,y}} = \abs{\abr{cy,y}} = \abs{c\abr{y,y}} = \abs{c\norm{y}^2} = \br{\abs{c\norm{y}}}\norm{y} \\=\br{\abs{c\norm{c^{-1}x}}}\norm{y} = \br{\abs{c\sqrt{\abr{c^{-1}x, c^{-1}x}}}}\norm{y} = \br{\abs{\sqrt{\abr{cx, c^{-1}x}}}}\norm{y} = \br{\abs{\sqrt{cc^{-1}\abr{x, x}}}}\norm{y} \\ = \norm{x}\cdot \norm{y} \end{multline*}

  Conversely, suppose $\abs{\abr{x,y}} = \norm{x}\cdot \norm{y}$. Then define $z = x-ay$, where $a = \frac{\abr{x,y}}{\norm{y}^2}$. Then we can show that $y$ and $z$ are orthogonal, which is when $\abr{z,y} = 0$. $$\abr{z,y} = \abr{x-ay,y} = \abr{x,y} - a\abr{y,y} = \abr{x,y}-\frac{\abr{x,y}\norm{y}^2}{\norm{y}^2} = 0.$$ Furthermore, we can also deduce that $$\abs{a} = \abs{\frac{\abr{x,y}}{\norm{y}^2}} = \frac{\abs{\abr{x,y}}}{\norm{y}^2} = \frac{\norm{x}\cdot \norm{y}}{\norm{y}^2} = \frac{\norm{x}}{\norm{y}}$$ 
  
  Since these vectors are orthogonal (any scalar multiple of $y$ will still be orthogonal to $z$), we can use the Pythagorean theorem for inner product spaces to find (note that $x = ay+z$) that $$\norm{x}^2 = \norm{ay+z}^2 = \norm{ay}^2 + \norm{z}^2 = \abs{a}^2\norm{y}^2 + \norm{z}^2 = \frac{\norm{x}^2\norm{y}^2}{\norm{y}^2} + \norm{z}^2 = \norm{x}^2 + \norm{z}^2.$$ This implies that $\norm{z}^2 = 0 \iff \norm{z} = 0 \iff z = \vec{0}$, and so $\vec{0} = x-ay \iff x = ay$.

  Hence $\abs{\abr{x,y}} = \norm{x}\cdot \norm{y}$ if and only if one of the vectors $x$ or $y$ is a multiple of the other.
\end{proof}

(b) Derive a similar result for the equality $\norm{x+y} = \norm{x} + \norm{y}$, and generalize it to the case of $n$ vectors.

\begin{theorem*}
  Let $x,y$ be vectors in an inner product space $\mathsf{V}$. Then $\norm{x+y} = \norm{x} + \norm{y}$ if and only if one vector is a nonnegative scalar multiple of the other vector; that is, if $x=cy$ or $y=cx$ for $c\geq 0$.

  When considering the norm of a sum of $n$ vectors, all of these vectors must be nonnegative scalar multiples of each other.
\end{theorem*}

\begin{proof}
  Induction on $n$ (forwards direction, suppose that all of the vectors are nonnegative scalar multiples of each other), and the base case where $n=2$ and the inductive step are carried out via definition of the norm. The converse is harder but it is similarly a proof by induction (one decomposition is enough, since the remaining vectors will be a sum of less than $n$ vectors).
\end{proof}

17. Let $\mathsf{T}$ be a linear operator on an inner product space $\mathsf{V}$, and suppose that $\norm{\mathsf{T}(x)} = \norm{x}$ for all $x$. Prove that $\mathsf{T}$ is one-to-one.

\begin{proof}
  Let $\mathsf{T}$ be a linear operator on an inner product space $\mathsf{V}$, and suppose that $\norm{\mathsf{T}(x)} = \norm{x}$ for all $x$.

  Then suppose that $\mathsf{T}(x) = \mathsf{T}(y)$ for $x,y\in\mathsf{V}$. Then $\norm{x} = \norm{\mathsf{T}(x)} = \norm{\mathsf{T}(y)} \implies \norm{x}^2 = \norm{\mathsf{T}(x)}^2 = \norm{\mathsf{T}(y)}^2$. So by definition $\norm{x}^2 = \abr{\mathsf{T}(x), \mathsf{T}(x)} = \abr{\mathsf{T}(y), \mathsf{T}(y)}$, and since $\mathsf{T}(x) = \mathsf{T}(y)$, write this last equality instead as $\abr{\mathsf{T}(x), \mathsf{T}(x)} = \abr{\mathsf{T}(x), \mathsf{T}(y)}$ and $\abr{\mathsf{T}(y), \mathsf{T}(x)} = \abr{\mathsf{T}(y), \mathsf{T}(y)}$. Then subtract the two equations from each other to find that $$\abr{\mathsf{T}(x), \mathsf{T}(x)} - \abr{\mathsf{T}(y), \mathsf{T}(x)} = \abr{\mathsf{T}(x), \mathsf{T}(y)} - \abr{\mathsf{T}(y), \mathsf{T}(y)}$$ $$\iff \abr{\mathsf{T}(x)-\mathsf{T}(y), \mathsf{T}(x)} - \abr{\mathsf{T}(x)-\mathsf{T}(y), \mathsf{T}(y)} = 0 $$ $$\implies \abr{\mathsf{T}(x)-\mathsf{T}(y), \mathsf{T}(x)-\mathsf{T}(y)} = 0 \iff \abr{\mathsf{T}(x-y), \mathsf{T}(x-y)} = 0$$ $$\implies \norm{\mathsf{T}(x-y)}^2 = 0 = \norm{x-y}^2 = \abr{x-y,x-y} \iff x-y = 0 \iff x=y$$

  So when $\mathsf{T}(x) = \mathsf{T}(y)$, $x=y$; this implies $\mathsf{T}$ is one-to-one.  
\end{proof}

\end{document}